{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3mTdWL-AhzW",
        "outputId": "eea4746e-84aa-42ed-d874-c627e4bd2ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders DataFrame Schema:\n",
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- supplier_id: string (nullable = true)\n",
            " |-- delivery_date: date (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- delay_days: integer (nullable = true)\n",
            " |-- is_delayed: integer (nullable = true)\n",
            " |-- total_value: double (nullable = true)\n",
            "\n",
            "\n",
            "Delayed Shipments Preview:\n",
            "+--------+-----------+-------------+-------+----------+----------+-----------+\n",
            "|order_id|supplier_id|delivery_date| status|delay_days|is_delayed|total_value|\n",
            "+--------+-----------+-------------+-------+----------+----------+-----------+\n",
            "|     103|       S001|   2025-10-23|Delayed|         2|         1|     2500.0|\n",
            "+--------+-----------+-------------+-------+----------+----------+-----------+\n",
            "\n",
            "\n",
            "Grouped Results: Supplier Performance (Delayed Orders)\n",
            "+-----------+-------------------+-------------------+\n",
            "|supplier_id|Delayed_Order_Count|Total_Delayed_Value|\n",
            "+-----------+-------------------+-------------------+\n",
            "|       S001|                  1|             2500.0|\n",
            "+-----------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "Processing Complete. Output saved to: data/processed/supplier_delay_metrics_*\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, when, count\n",
        "\n",
        "# 1. Initialize Spark Session\n",
        "# This is the entry point for all PySpark functionality\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SupplyChainPySparkProcessor\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- 2. Load Data from CSV ---\n",
        "\n",
        "# Define the path to your CSV file.\n",
        "# NOTE: Replace 'path/to/' with your actual file path.\n",
        "# We infer the schema to automatically detect data types.\n",
        "file_path = \"path/to/processed_orders.csv\"\n",
        "\n",
        "# In a real environment, you'd load the cleaned data from Week 2.\n",
        "# Since we don't have that file here, we'll create a dummy DataFrame\n",
        "# and save it to simulate the load.\n",
        "\n",
        "# --- SIMULATION START: Creating dummy CSV to ensure script runs ---\n",
        "# This part simulates the output of Week 2 to make Week 3 runnable.\n",
        "data = [\n",
        "    (101, \"S001\", \"2025-10-15\", \"Delivered\", 0, 0, 1500.00),\n",
        "    (102, \"S002\", \"2025-10-28\", \"Pending\", -3, 0, 2750.00), # Not delayed yet\n",
        "    (103, \"S001\", \"2025-10-23\", \"Delayed\", 2, 1, 2500.00), # Delayed (Expected 2 days ago)\n",
        "    (105, \"S002\", \"2025-10-01\", \"Delivered\", 0, 0, 550.00)\n",
        "]\n",
        "columns = [\"order_id\", \"supplier_id\", \"delivery_date\", \"status\", \"delay_days\", \"is_delayed\", \"total_value\"]\n",
        "dummy_df = spark.createDataFrame(data, columns)\n",
        "dummy_df.write.mode(\"overwrite\").csv(\"data/raw_orders_pyspark.csv\", header=True)\n",
        "# --- SIMULATION END ---\n",
        "\n",
        "# Now, load the data from the simulated CSV\n",
        "orders_df = spark.read.csv(\"data/raw_orders_pyspark.csv\", header=True, inferSchema=True)\n",
        "print(\"Orders DataFrame Schema:\")\n",
        "orders_df.printSchema()\n",
        "\n",
        "\n",
        "# --- 3. Filter Delayed Shipments ---\n",
        "# We filter for records where 'is_delayed' is 1 (True)\n",
        "delayed_shipments_df = orders_df.filter(col(\"is_delayed\") == 1)\n",
        "\n",
        "print(\"\\nDelayed Shipments Preview:\")\n",
        "delayed_shipments_df.show(5)\n",
        "\n",
        "\n",
        "# --- 4. Group by Supplier and Count Delayed Orders ---\n",
        "# Aggregation to find how many delayed orders each supplier has\n",
        "delayed_counts_by_supplier = delayed_shipments_df.groupBy(\"supplier_id\") \\\n",
        "    .agg(\n",
        "        count(col(\"order_id\")).alias(\"Delayed_Order_Count\"),\n",
        "        sum(col(\"total_value\")).alias(\"Total_Delayed_Value\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"Delayed_Order_Count\").desc())\n",
        "\n",
        "print(\"\\nGrouped Results: Supplier Performance (Delayed Orders)\")\n",
        "delayed_counts_by_supplier.show()\n",
        "\n",
        "\n",
        "# --- 5. Save Processed Data to CSV (or Parquet) ---\n",
        "output_path = \"data/processed/supplier_delay_metrics\"\n",
        "\n",
        "# Save the grouped results as Parquet (optimized for subsequent Spark reads)\n",
        "delayed_counts_by_supplier.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(output_path + \"_parquet\")\n",
        "\n",
        "# Save the grouped results as CSV (for easy human review - the Deliverable \"Output file\")\n",
        "delayed_counts_by_supplier.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .csv(output_path + \"_csv\", header=True)\n",
        "\n",
        "print(f\"\\nProcessing Complete. Output saved to: {output_path}_*\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ]
}